# -*- coding: utf-8 -*-
"""LSTM_Training.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zFYUwxm4qMruKd1aHMacya41-M_7qyCz

##Imports
"""

import numpy as np
import pandas as pd
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
import matplotlib.pyplot as plt
import os
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error
from sklearn.preprocessing import MinMaxScaler

from google.colab import drive
drive.mount('/content/drive')

cd /content/drive/MyDrive/exceliles

files = os.listdir()
print(files)

"""##Data processing"""

def process_xlsx_file(file_path):
    # Read XLSX file and extract 'data sheet'
    xl = pd.ExcelFile(file_path)
    sheet_name = 'Data Sheet'

    if sheet_name not in xl.sheet_names:
        return None

    df = xl.parse(sheet_name)
    price_row = df[df['COMPANY NAME'].str.contains('PRICE:', na=False)]
    price_list = price_row.values.flatten().tolist()
    price_list = [value for value in price_list if pd.notna(value)]

    if len(price_list) < 10:
        return None
    return price_list[1:]

# Iterate through all XLSX files in the folder
prices_data = []
for filename in os.listdir():
    if filename.endswith(".xlsx"):
        file_path = os.path.join(filename)

        # Process the XLSX file
        prices = process_xlsx_file(file_path)

        # Check if valid data is obtained
        if prices is not None:
            print(f"File: {filename}, Prices: {prices}")
            prices_data.append(prices)

def min_max_normalize(list_of_lists):
    normalized_data = []

    for data_list in list_of_lists:
        min_val = min(data_list)
        max_val = max(data_list)

        normalized_list = [(x - min_val) / (max_val - min_val) for x in data_list]
        normalized_data.append(normalized_list)

    return normalized_data

prices_data_normalised = min_max_normalize(prices_data)

"""##Linear Regression"""

def stock_price_predictor(prices):
    np.random.seed(42)
    num_days = len(prices)
    days = np.arange(1, num_days + 1)

    # Creating a DataFrame for easy manipulation
    df = pd.DataFrame({'Day': days, 'Price': prices})

    # Feature engineering: Adding a lag feature
    df['Previous_Price'] = df['Price'].shift(1)

    # Drop NaN values introduced by the lag
    df = df.dropna()

    # Features and target variable
    X = df[['Day', 'Previous_Price']]
    y = df['Price']

    # Split the data into training and testing sets
    split_ratio = 0.8
    split_index = int(len(df) * split_ratio)
    X_train, X_test = X[:split_index], X[split_index:]
    y_train, y_test = y[:split_index], y[split_index:]

    # Train a linear regression model
    model = LinearRegression()
    model.fit(X_train, y_train)

    # Make predictions on the test set
    y_pred = model.predict(X_test)

    # Evaluate the model
    mse = mean_squared_error(y_test, y_pred)
    #print(f'Mean Squared Error on Test Set: {mse:.2f}')

    # # Plot the results
    # plt.plot(X['Day'], y, color='black', label='Actual Prices')
    # plt.plot(X_test['Day'], y_pred, color='red', linewidth=3, linestyle='--', label='Predicted Prices')
    # plt.xlabel('Day')
    # plt.ylabel('Stock Price')
    # plt.title('Stock Price Prediction')
    # plt.legend()
    # plt.show()
    # print('Actual price and Prediction: ',(y_test[num_days-1], y_pred[-1]))
    return(y_test[num_days-1], y_pred[-1])

result = []
for prices in prices_data_normalised:
  result.append(stock_price_predictor(prices))

from sklearn.metrics import mean_squared_error

def calculate_mse(actual_predicted_tuples):
    # Unpack the tuples into separate lists
    actual_values, predicted_values = zip(*actual_predicted_tuples)

    # Calculate the Mean Squared Error
    mse = mean_squared_error(actual_values, predicted_values)

    return mse

mse_result = calculate_mse(result)

print(f"Mean Squared Error: {mse_result:.2f}")

print(result)

sample_prices = np.cumsum(np.random.normal(loc=0.5, scale=2, size=100)) + 100
stock_price_predictor(sample_prices)

def find_longest_list(list_of_lists):
    return max(list_of_lists, key=len)
stock_price_predictor(find_longest_list(prices_data_normalised))



"""##Updated Baseline"""

df = pd.read_csv('nifty.csv')
#df = pd.read_csv('/content/drive/MyDrive/IR Project/NIFTY 50-27-03-2023-to-27-03-2024.csv')

data = df

data.head()

# Data Preprocessing
data['Date'] = pd.to_datetime(data['Date '])

# Feature Engineering (if necessary)

# Splitting data into features and target variable
X = data[['Open ', 'High ', 'Low ', 'Close ', 'Shares Traded ']]
y = data['Close ']
# Splitting data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
# Model Selection and Training
model = RandomForestRegressor(random_state=42)
model.fit(X_train, y_train)

# Model Evaluation
y_pred = model.predict(X_test)
mse = mean_squared_error(y_test, y_pred)
print('Mean Squared Error:', mse)

# Prediction
next_price_prediction = model.predict(X_test.iloc[-1].values.reshape(1, -1))
print('Predicted Next Price:', next_price_prediction[0])

SEQ_LENGTH = 3  # Adjust this according to your sequence length preference

# Prepare data for RNN
def create_sequences(data, seq_length):
    sequences = []
    for i in range(len(data) - seq_length):
        sequence = data[i:i+seq_length+1]  # Adjusted to include the next price as well
        sequences.append(sequence)
    return np.array(sequences)

scaler = MinMaxScaler(feature_range=(0, 1))
prices = df[['Open ', 'High ', 'Low ', 'Close ']].values
prices_scaled = scaler.fit_transform(prices)
BATCH_SIZE=64

X = create_sequences(prices_scaled, SEQ_LENGTH)
y = X[:, -1]  # Get the last element of each sequence as y
X = X[:, :-1]  # Remove the last element from each sequence to form X

print(X.shape, y.shape)

# Split the data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Convert to PyTorch tensors
X_train_tensor = torch.tensor(X_train, dtype=torch.float32)
y_train_tensor = torch.tensor(y_train, dtype=torch.float32)
X_test_tensor = torch.tensor(X_test, dtype=torch.float32)
y_test_tensor = torch.tensor(y_test, dtype=torch.float32)

# Create DataLoader for training and testing
train_dataset = TensorDataset(X_train_tensor, y_train_tensor)
train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)
test_dataset = TensorDataset(X_test_tensor, y_test_tensor)
test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)  # Set shuffle to False for testing

# Calculate accuracy for regression task
def calculate_accuracy(predictions, targets, threshold):
    correct_predictions = np.abs(predictions - targets) <= threshold
    accuracy = np.mean(correct_predictions)
    return accuracy

# Evaluate the model
def evaluate_model(model, data_loader, threshold=0.05):
    model.eval()
    accuracies = []
    for batch_X, batch_y in data_loader:
        output = model(batch_X)
        predictions = output.detach().numpy()
        targets = batch_y.detach().numpy()
        accuracy = calculate_accuracy(predictions, targets, threshold)
        accuracies.append(accuracy)
    avg_accuracy = np.mean(accuracies)
    return avg_accuracy

import numpy as np

def evaluate_model_mpa(model, data_loader, threshold=0.05):
    model.eval()
    mean_prediction_accuracies = []

    for batch_X, batch_y in data_loader:
        output = model(batch_X)
        predictions = output.detach().numpy()
        targets = batch_y.detach().numpy()

        # Calculate mean prediction accuracy for the current batch
        mean_prediction_accuracy = 1 - (np.abs(targets - predictions) / np.abs(targets)).mean()
        mean_prediction_accuracies.append(mean_prediction_accuracy)

    # Calculate the average mean prediction accuracy over all batches
    avg_mean_prediction_accuracy = np.mean(mean_prediction_accuracies)

    return avg_mean_prediction_accuracy


# threshold = 0.05
# test_accuracy = evaluate_model(model, test_loader, threshold)
# print(f'Testing Accuracy: {test_accuracy:.4f}')

import numpy as np
import pandas as pd
import torch
import torch.nn as nn
from torch.utils.data import DataLoader, TensorDataset
from sklearn.preprocessing import MinMaxScaler

# # Convert date to numeric values (day of the year)
# def date_to_numeric(date):
#     return int(date.split("-")[0])

# # Preprocess the data
# df['Date '] = pd.to_datetime(df['Date '])  # Convert date column to datetime
# df['Day_of_Year '] = df['Date '].dt.dayofyear  # Extract day of the year
# prices = df[['Open ', 'High ', 'Low ', 'Close ']].values
# volume = df['Shares Traded '].values

# scaler = MinMaxScaler(feature_range=(0, 1))
# prices_scaled = scaler.fit_transform(prices)

# # Prepare data for RNN
# def create_sequences(data, seq_length):
#     sequences = []
#     for i in range(len(data) - seq_length):
#         sequence = data[i:i+seq_length]
#         sequences.append(sequence)
#     return np.array(sequences)

# SEQ_LENGTH = 3  # Adjust this according to your sequence length preference
# X = create_sequences(prices_scaled, SEQ_LENGTH)
# y = prices_scaled[SEQ_LENGTH:]
# print(X.shape,y.shape)

# # Convert to PyTorch tensors
# X_tensor = torch.tensor(X, dtype=torch.float32)
# y_tensor = torch.tensor(y, dtype=torch.float32)

# Define RNN model
class StockRNN(nn.Module):
    def __init__(self, input_size, hidden_size, num_layers, output_size):
        super(StockRNN, self).__init__()
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)
        self.fc = nn.Linear(hidden_size, output_size)

    def forward(self, x):
        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)
        out, _ = self.rnn(x, h0)
        out = self.fc(out[:, -1, :])
        return out

# Initialize model, loss function, and optimizer
INPUT_SIZE = 4
HIDDEN_SIZE = 32
NUM_LAYERS = 2
OUTPUT_SIZE = 4
model = StockRNN(INPUT_SIZE, HIDDEN_SIZE, NUM_LAYERS, OUTPUT_SIZE)
criterion = nn.MSELoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

# Train the model
NUM_EPOCHS = 100
BATCH_SIZE = 64
train_dataset = TensorDataset(X_train_tensor, y_train_tensor)
train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)

for epoch in range(NUM_EPOCHS):
    for batch_X, batch_y in train_loader:
        output = model(batch_X)
        loss = criterion(output, batch_y)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
    if (epoch+1) % 10 == 0:
        print(f'Epoch [{epoch+1}/{NUM_EPOCHS}], Loss: {loss.item():.4f}')


def predict_next_prices(model, last_prices):
    with torch.no_grad():
        last_prices_scaled = scaler.transform(last_prices)
        last_prices_tensor = torch.tensor(last_prices_scaled, dtype=torch.float32).unsqueeze(0)
        predicted_prices = model(last_prices_tensor).numpy()
        return scaler.inverse_transform(predicted_prices)[0]

last_prices = prices[-SEQ_LENGTH:]
next_prices = predict_next_prices(model, last_prices)
print("Predicted next prices:", next_prices)
print(last_prices)

threshold = 0.05
test_accuracy = evaluate_model(model, test_loader, threshold)
print(f'Testing Accuracy: {test_accuracy:.4f}')

import matplotlib.pyplot as plt

# Define LSTM model
class StockLSTM(nn.Module):
    def __init__(self, input_size, hidden_size, num_layers, output_size):
        super(StockLSTM, self).__init__()
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)
        self.fc = nn.Linear(hidden_size, output_size)

    def forward(self, x):
        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)
        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)
        out, _ = self.lstm(x, (h0, c0))
        out = self.fc(out[:, -1, :])
        return out

INPUT_SIZE = 4
HIDDEN_SIZE = 32
NUM_LAYERS = 2
OUTPUT_SIZE = 4

# Train the model
NUM_EPOCHS = 1000
BATCH_SIZE = 64
train_dataset = TensorDataset(X_train_tensor, y_train_tensor)
train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)

# Initialize LSTM model, loss function, and optimizer
model = StockLSTM(INPUT_SIZE, HIDDEN_SIZE, NUM_LAYERS, OUTPUT_SIZE)
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

# Function to plot training loss
def plot_loss(loss_history):
    plt.plot(loss_history, label='Training Loss')
    plt.title('Training Loss over Epochs')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()
    plt.show()

# Function to plot actual and predicted prices
def plot_prices(actual_prices, predicted_prices):
    plt.plot(actual_prices, label='Actual Prices', marker='o')
    plt.plot(predicted_prices, label='Predicted Prices', marker='x')
    plt.title('Actual vs. Predicted Prices')
    plt.xlabel('Time')
    plt.ylabel('Prices')
    plt.legend()
    plt.show()

# Initialize an empty list to store the training loss
loss_history = []

loss_history = []
test_accuracy_history = []

# Train the LSTM model
for epoch in range(NUM_EPOCHS):
    epoch_losses = []
    for batch_X, batch_y in train_loader:
        output = model(batch_X)
        loss = criterion(output, batch_y)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        epoch_losses.append(loss.item())

    # Calculate average epoch loss
    avg_epoch_loss = sum(epoch_losses) / len(epoch_losses)
    loss_history.append(avg_epoch_loss)

    # Evaluate model on test set to get test accuracy
    test_accuracy = evaluate_model(model, test_loader, threshold)
    test_accuracy_history.append(test_accuracy)

    # Print and plot
    if (epoch+1) % 100 == 0:
        print(f'Epoch [{epoch+1}/{NUM_EPOCHS}], Loss: {avg_epoch_loss:.4f}, Test Accuracy: {test_accuracy:.4f}')

# Plot training loss and test accuracy
plt.plot(loss_history, label='Training Loss')
plt.plot(test_accuracy_history, label='Test Accuracy')
plt.title('Training Loss and Test Accuracy over Epochs')
plt.xlabel('Epochs')
plt.ylabel('Value')
plt.legend()
plt.yscale('log')  # Use logarithmic scale for y-axis

# Add text annotations for final loss and accuracy
final_loss = loss_history[-1]
final_accuracy = test_accuracy_history[-1]
plt.text(len(loss_history) - 1, final_loss, f'Final Loss: {final_loss:.4f}', ha='right')
plt.text(len(test_accuracy_history) - 1, final_accuracy, f'Final Accuracy: {final_accuracy:.4f}', ha='right')

plt.show()


# Make predictions using LSTM model
def predict_next_prices_lstm(model, last_prices):
    with torch.no_grad():
        last_prices_scaled = scaler.transform(last_prices)
        last_prices_tensor = torch.tensor(last_prices_scaled, dtype=torch.float32).unsqueeze(0)
        predicted_prices = model(last_prices_tensor).numpy()
        return scaler.inverse_transform(predicted_prices)[0]

last_prices = prices[-SEQ_LENGTH:]
next_prices_lstm = predict_next_prices_lstm(model, last_prices)

# Plot actual and predicted prices
#plot_prices(prices[-SEQ_LENGTH:], next_prices_lstm)

print(evaluate_model_mpa(model, test_loader, threshold))

import pickle

# Assuming your model is named 'model'
# Save the model to a file using pickle
with open('model.pkl', 'wb') as f:
    pickle.dump(model, f)

threshold = 0.05
test_accuracy = evaluate_model(model, test_loader, threshold)
print(f'Testing Accuracy: {test_accuracy:.4f}')



import torch.nn.functional as F
from torch.nn import TransformerEncoder, TransformerEncoderLayer

# Define Transformer model
class StockTransformer(nn.Module):
    def __init__(self, input_size, hidden_size, num_layers, output_size):
        super(StockTransformer, self).__init__()
        self.hidden_size = hidden_size
        self.encoder_layers = TransformerEncoderLayer(d_model=input_size, nhead=1)
        self.transformer_encoder = TransformerEncoder(self.encoder_layers, num_layers=num_layers)
        self.fc = nn.Linear(input_size, output_size)

    def forward(self, x):
        x = x.transpose(0, 1)  # (seq_len, batch_size, input_size) for Transformer
        out = self.transformer_encoder(x)
        out = self.fc(out[-1, :, :])
        return out

# Initialize Transformer model, loss function, and optimizer
model = StockTransformer(INPUT_SIZE, HIDDEN_SIZE, NUM_LAYERS, OUTPUT_SIZE)
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

NUM_EPOCHS=300

# Train the Transformer model
for epoch in range(NUM_EPOCHS):
    for batch_X, batch_y in train_loader:
        output = model(batch_X)
        loss = criterion(output, batch_y)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
    if (epoch+1) % 10 == 0:
        print(f'Epoch [{epoch+1}/{NUM_EPOCHS}], Loss: {loss.item():.4f}')

# Make predictions using Transformer model
def predict_next_prices_transformer(model, last_prices):
    with torch.no_grad():
        last_prices_scaled = scaler.transform(last_prices)
        last_prices_tensor = torch.tensor(last_prices_scaled, dtype=torch.float32).unsqueeze(0)
        predicted_prices = model(last_prices_tensor).numpy()
        return scaler.inverse_transform(predicted_prices)[0]

last_prices = prices[-SEQ_LENGTH:]
next_prices_transformer = predict_next_prices_transformer(model, last_prices)
print("Predicted next prices using Transformer:", next_prices_transformer)

threshold = 0.05
test_accuracy = evaluate_model(model, test_loader, threshold)
print(f'Training Accuracy: {test_accuracy:.4f}')